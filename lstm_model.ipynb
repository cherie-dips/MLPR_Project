{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5056bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split complete: Each split contains all folders from all time steps, with different images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "source_dir = \"Preprocessed_Data\"\n",
    "target_dir = \"Split_Data\"\n",
    "splits = ['train', 'val', 'test']\n",
    "split_ratios = {'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "\n",
    "# For each time folder (e.g., \"0 hr\", \"24 hr\", ...)\n",
    "for time_folder in os.listdir(source_dir):\n",
    "    time_path = os.path.join(source_dir, time_folder)\n",
    "    if not os.path.isdir(time_path):\n",
    "        continue\n",
    "\n",
    "    # For each condition folder inside each time step\n",
    "    for cond_folder in os.listdir(time_path):\n",
    "        cond_path = os.path.join(time_path, cond_folder)\n",
    "        if not os.path.isdir(cond_path):\n",
    "            continue\n",
    "\n",
    "        # List and shuffle images\n",
    "        images = [f for f in os.listdir(cond_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        random.seed(42)\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Calculate split indices\n",
    "        n = len(images)\n",
    "        n_train = int(split_ratios['train'] * n)\n",
    "        n_val = int(split_ratios['val'] * n)\n",
    "        n_test = n - n_train - n_val\n",
    "\n",
    "        split_files = {\n",
    "            'train': images[:n_train],\n",
    "            'val': images[n_train:n_train + n_val],\n",
    "            'test': images[n_train + n_val:]\n",
    "        }\n",
    "\n",
    "        # Copy files to target structure\n",
    "        for split in splits:\n",
    "            dest_folder = os.path.join(target_dir, split, time_folder, cond_folder)\n",
    "            os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "            for img in split_files[split]:\n",
    "                src_img_path = os.path.join(cond_path, img)\n",
    "                dest_img_path = os.path.join(dest_folder, img)\n",
    "                shutil.copy2(src_img_path, dest_img_path)\n",
    "\n",
    "print(\"✅ Split complete: Each split contains all folders from all time steps, with different images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816a72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a095d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydrogelSequenceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = {}  # condition: list of (time, path)\n",
    "        \n",
    "        # Iterate over the time folders\n",
    "        time_folders = sorted(os.listdir(root_dir), key=lambda x: int(x.split()[0]))\n",
    "\n",
    "        for time_folder in time_folders:\n",
    "            t = int(time_folder.split()[0].replace(\"hr\", \"\").strip())  # Get the time (e.g., 0, 5)\n",
    "            time_path = os.path.join(root_dir, time_folder)\n",
    "            \n",
    "            # Iterate over condition folders inside each time folder\n",
    "            for cond_folder in os.listdir(time_path):\n",
    "                cond_path = os.path.join(time_path, cond_folder)\n",
    "                if not os.path.isdir(cond_path):\n",
    "                    continue\n",
    "\n",
    "                # Make the condition key based on the folder name\n",
    "                key = cond_folder.strip().lower()\n",
    "                \n",
    "                # Add images to the corresponding condition\n",
    "                for img_file in os.listdir(cond_path):\n",
    "                    if img_file.endswith(('.JPG', '.jpeg', '.png')):\n",
    "                        path = os.path.join(cond_path, img_file)\n",
    "                        if key not in self.samples:\n",
    "                            self.samples[key] = []\n",
    "                        self.samples[key].append((t, path))\n",
    "\n",
    "        # Create a list of sequences (sorted by time)\n",
    "        self.sequence_data = []\n",
    "        for cond, lst in self.samples.items():\n",
    "            lst.sort()  # sort by time\n",
    "            imgs = [path for _, path in lst]\n",
    "            self.sequence_data.append((cond, imgs))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cond, paths = self.sequence_data[idx]\n",
    "        imgs = []\n",
    "        for path in paths:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            imgs.append(img)\n",
    "\n",
    "        images = torch.stack(imgs)  # shape: [seq_len, 3, H, W]\n",
    "\n",
    "        # extract labels from cond\n",
    "        parts = cond.split()\n",
    "        pH = float(parts[0].replace(\"ph\", \"\"))  # Example: \"pH5\" -> 5.0\n",
    "        remaining_time = 140 - (len(images) - 1) * 5  # Assuming 5hr steps for remaining time\n",
    "\n",
    "        return images, torch.tensor(pH), torch.tensor(remaining_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9d1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # remove final FC\n",
    "        self.feature_dim = resnet.fc.in_features\n",
    "\n",
    "        self.lstm = nn.LSTM(self.feature_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        self.ph_head = nn.Linear(hidden_dim, 1)      # Predict pH\n",
    "        self.time_head = nn.Linear(hidden_dim, 1)    # Predict remaining time\n",
    "\n",
    "    def forward(self, x_seq):  # x_seq: [B, T, 3, H, W]\n",
    "        B, T, C, H, W = x_seq.size()\n",
    "        x_seq = x_seq.view(B * T, C, H, W)\n",
    "\n",
    "        # Extract features for each frame using ResNet\n",
    "        with torch.no_grad():  # Freeze ResNet during LSTM training (optional)\n",
    "            features = self.feature_extractor(x_seq)  # [B*T, feat_dim, 1, 1]\n",
    "            features = features.view(B, T, -1)        # [B, T, feat_dim]\n",
    "\n",
    "        # Feed sequence of features into LSTM\n",
    "        lstm_out, _ = self.lstm(features)             # [B, T, hidden_dim]\n",
    "\n",
    "        # Take the last time step's output\n",
    "        last_hidden = lstm_out[:, -1, :]              # [B, hidden_dim]\n",
    "\n",
    "        # Predict pH and time-to-degradation\n",
    "        ph_pred = self.ph_head(last_hidden)           # [B, 1]\n",
    "        time_pred = self.time_head(last_hidden)       # [B, 1]\n",
    "\n",
    "        return ph_pred.squeeze(1), time_pred.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed6870ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to ResNet input size\n",
    "    transforms.ToTensor(),          # Convert PIL Image to tensor\n",
    "    transforms.Normalize(           # Normalize as per ImageNet\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = HydrogelSequenceDataset(\"Split_Data/train\", transform=transform)\n",
    "val_dataset = HydrogelSequenceDataset(\"Split_Data/val\", transform=transform)\n",
    "test_dataset = HydrogelSequenceDataset(\"Split_Data/test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea3ddb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [294, 3, 224, 224] at entry 0 and [287, 3, 224, 224] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m  \u001b[38;5;66;03m# Track loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Iterate through the training data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mph_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Move data to GPU or CPU\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mph_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mph_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [294, 3, 224, 224] at entry 0 and [287, 3, 224, 224] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device: use GPU if available, otherwise fall back to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ResNetLSTM(hidden_dim=128, num_layers=1).to(device)  # Move model to device\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set the number of epochs for training\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Training mode\n",
    "    running_loss = 0.0  # Track loss\n",
    "\n",
    "    # Iterate through the training data\n",
    "    for images, ph_labels, time_labels in train_loader:\n",
    "        images = images.to(device)  # Move data to GPU or CPU\n",
    "        ph_labels = ph_labels.to(device)\n",
    "        time_labels = time_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "        # Forward pass\n",
    "        ph_pred, time_pred = model(images)\n",
    "\n",
    "        # Calculate losses for pH and time\n",
    "        ph_loss = criterion(ph_pred, ph_labels)\n",
    "        time_loss = criterion(time_pred, time_labels)\n",
    "        loss = ph_loss + time_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Update model weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss for this epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "torch.save(model.state_dict(), 'resnet_lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed121db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1672259.1250\n",
      "Epoch [2/20], Loss: 1670240.6250\n",
      "Epoch [3/20], Loss: 1668567.1250\n",
      "Epoch [4/20], Loss: 1667194.2500\n",
      "Epoch [5/20], Loss: 1666039.2500\n",
      "Epoch [6/20], Loss: 1665021.7500\n",
      "Epoch [7/20], Loss: 1664086.7500\n",
      "Epoch [8/20], Loss: 1663196.7500\n",
      "Epoch [9/20], Loss: 1662329.2500\n",
      "Epoch [10/20], Loss: 1661487.2500\n",
      "Epoch [11/20], Loss: 1660671.6250\n",
      "Epoch [12/20], Loss: 1659863.6250\n",
      "Epoch [13/20], Loss: 1659062.5000\n",
      "Epoch [14/20], Loss: 1658294.2500\n",
      "Epoch [15/20], Loss: 1657542.7500\n",
      "Epoch [16/20], Loss: 1656789.8750\n",
      "Epoch [17/20], Loss: 1656037.3750\n",
      "Epoch [18/20], Loss: 1655320.0000\n",
      "Epoch [19/20], Loss: 1654662.1250\n",
      "Epoch [20/20], Loss: 1654060.3750\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ======== Custom collate function for LSTM ========\n",
    "def collate_fn_lstm(batch):\n",
    "    sequences, ph_labels, time_labels = zip(*batch)\n",
    "\n",
    "    # Stack image sequences [seq_len, 3, 224, 224]\n",
    "    # sequences = [torch.stack(seq) for seq in sequences]  # List of tensors\n",
    "\n",
    "    # Pad sequences to same length\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True)  # [B, T, 3, 224, 224]\n",
    "\n",
    "    # Labels\n",
    "    ph_labels = torch.tensor(ph_labels, dtype=torch.float32)\n",
    "    time_labels = torch.tensor(time_labels, dtype=torch.float32)\n",
    "\n",
    "    return padded_seqs, ph_labels, time_labels\n",
    "\n",
    "# ======== Loaders (example usage) ========\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn_lstm)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn_lstm)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn_lstm)\n",
    "\n",
    "# ======== Model definition example (ResNet + LSTM) ========\n",
    "class ResNetLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, num_layers=1):\n",
    "        super(ResNetLSTM, self).__init__()\n",
    "        from torchvision.models import resnet18\n",
    "        resnet = resnet18(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC\n",
    "        self.feature_dim = 512  # ResNet18 final feature dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.feature_dim, hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.fc_ph = nn.Linear(hidden_dim, 1)\n",
    "        self.fc_time = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, 3, 224, 224]\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        with torch.no_grad():  # Freeze CNN if desired\n",
    "            cnn_feats = self.cnn(x).view(B, T, -1)  # [B, T, F]\n",
    "        lstm_out, _ = self.lstm(cnn_feats)  # [B, T, hidden_dim]\n",
    "        last_hidden = lstm_out[:, -1, :]  # Use last time step\n",
    "        ph_pred = self.fc_ph(last_hidden).squeeze(1)\n",
    "        time_pred = self.fc_time(last_hidden).squeeze(1)\n",
    "        return ph_pred, time_pred\n",
    "\n",
    "# ======== Training Setup ========\n",
    "model = ResNetLSTM(hidden_dim=128, num_layers=1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 20\n",
    "\n",
    "# ======== Training Loop ========\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, ph_labels, time_labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        ph_labels = ph_labels.to(device)\n",
    "        time_labels = time_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        ph_pred, time_pred = model(images)\n",
    "\n",
    "        ph_loss = criterion(ph_pred, ph_labels)\n",
    "        time_loss = criterion(time_pred, time_labels)\n",
    "        loss = ph_loss + time_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ======== Save the Model ========\n",
    "torch.save(model.state_dict(), 'resnet_lstm_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
